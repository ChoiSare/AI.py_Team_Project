{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-587d513839f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[0mY\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     78\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[1;34m(*arrays, **options)\u001b[0m\n\u001b[0;32m   2120\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2121\u001b[0m     n_train, n_test = _validate_shuffle_split(n_samples, test_size, train_size,\n\u001b[1;32m-> 2122\u001b[1;33m                                               default_test_size=0.25)\n\u001b[0m\u001b[0;32m   2123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2124\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   1803\u001b[0m             \u001b[1;34m'resulting train set will be empty. Adjust any of the '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1804\u001b[0m             'aforementioned parameters.'.format(n_samples, test_size,\n\u001b[1;32m-> 1805\u001b[1;33m                                                 train_size)\n\u001b[0m\u001b[0;32m   1806\u001b[0m         )\n\u001b[0;32m   1807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.25 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "#from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "\n",
    "import os, re, glob\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2 \n",
    "\n",
    "#cv2.CascadeClassifier(cv2.data.haarcascades + \"haarcascade_frontalface_default.xml\")\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades +'haarcascade_frontalface_default.xml')\n",
    "groups_folder_path = r'C:\\Users\\ailab\\PycharmProjects\\face\\Bulk-Bing-Image-downloader-master\\AF'\n",
    "\n",
    "#categories = [\"0\", \"1\"]\n",
    "#num_classes = len(categories)\n",
    "\n",
    "image_w = 128\n",
    "image_h = 128\n",
    "  \n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "#\n",
    "image_dir = groups_folder_path + '/'\n",
    "idex = 0   \n",
    "label = []\n",
    "for top, dir, f in os.walk(image_dir):\n",
    "    print(len(f))\n",
    "    for filename in f:           \n",
    "        if 'angry' in filename :\n",
    "            print(\"a \", filename)\n",
    "            label = 0\n",
    "        elif 'neutral' in filename:  \n",
    "            print(\"n \", filename)\n",
    "            label = 1   \n",
    "        #elif 'happy' in filename:  \n",
    "            #print(\"h \", filename)\n",
    "            #label = 2 \n",
    "        #elif 'fearful' in filename:  \n",
    "            #print(\"f \", filename)\n",
    "            #label = 3\n",
    "        elif 'surprising' in filename:\n",
    "            print(\"su \", filename)\n",
    "            label = 4\n",
    "        elif 'sad' in filename:\n",
    "            print(\"s \", filename)\n",
    "            label = 5\n",
    "            \n",
    "        #print(image_dir+filename)\n",
    "        img = cv2.imread(image_dir+filename)\n",
    "        gray_img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        faces = face_cascade.detectMultiScale(gray_img, 1.03, 5)\n",
    "\n",
    "        if type(faces) == tuple:\n",
    "            continue\n",
    "        else:\n",
    "            if faces.shape[0] == 1:\n",
    "                for (x,y,w,h) in faces:\n",
    "                    cropped = gray_img[y:y + h, x:x + w]\n",
    "                re_gray = cv2.resize(cropped, None, fx=image_w/cropped.shape[0], fy=image_h/cropped.shape[1])            \n",
    "                #image.resize???              \n",
    "        X.append(re_gray)\n",
    "        Y.append(label)\n",
    "        #cv2.imshow('image', re_gray)\n",
    "        #cv2.waitKey(0)\n",
    "                    \n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y)\n",
    "print(X_train)\n",
    "print(Y_train)\n",
    "\n",
    "#cv2.imshow('image', X_train[0])\n",
    "#cv2.waitKey(0)\n",
    "#xy = (X_train, X_test, Y_train, Y_test)\n",
    "#cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.76080, saving model to ./model/01-0.7608.hdf5\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.76080 to 0.55224, saving model to ./model/02-0.5522.hdf5\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.55224 to 0.42229, saving model to ./model/03-0.4223.hdf5\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.42229 to 0.31668, saving model to ./model/04-0.3167.hdf5\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.31668 to 0.30421, saving model to ./model/05-0.3042.hdf5\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.30421 to 0.24142, saving model to ./model/06-0.2414.hdf5\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.24142 to 0.20743, saving model to ./model/07-0.2074.hdf5\n",
      "\n",
      "Epoch 00008: val_loss did not improve\n",
      "\n",
      "Epoch 00009: val_loss did not improve\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.20743 to 0.19804, saving model to ./model/10-0.1980.hdf5\n",
      "\n",
      "Epoch 00011: val_loss did not improve\n",
      "\n",
      "Epoch 00012: val_loss did not improve\n",
      "\n",
      "Epoch 00013: val_loss did not improve\n",
      "\n",
      "Epoch 00014: val_loss improved from 0.19804 to 0.19294, saving model to ./model/14-0.1929.hdf5\n",
      "\n",
      "Epoch 00015: val_loss did not improve\n",
      "\n",
      "Epoch 00016: val_loss did not improve\n",
      "\n",
      "Epoch 00017: val_loss did not improve\n",
      "\n",
      "Epoch 00018: val_loss did not improve\n",
      "\n",
      "Epoch 00019: val_loss did not improve\n",
      "\n",
      "Epoch 00020: val_loss did not improve\n",
      "\n",
      "Epoch 00021: val_loss did not improve\n",
      "\n",
      "Epoch 00022: val_loss did not improve\n",
      "\n",
      "Epoch 00023: val_loss did not improve\n",
      "\n",
      "Epoch 00024: val_loss did not improve\n"
     ]
    }
   ],
   "source": [
    "#(X_train, Y_class_train), (X_test, Y_class_test) = mnist.load_data()\n",
    "#X_train, X_test, Y_train, Y_test \n",
    "\n",
    "#총 100개의 원소가 들어있는 배열 x에 대해서 x.reshape(-1, 정수) 를 해주면 \n",
    "#'열(column)' 차원의 '정수'에 따라서 100개의 원소가 빠짐없이 배치될 수 있도록 \n",
    "#'-1'이 들어가 있는 '행(row)' 의 개수가 가변적으로 정해짐\n",
    "X_train = X_train.reshape(-1, 128, 128,1).astype('float64') / 255\n",
    "X_test = X_test.reshape(-1,128,128,1).astype('float64') / 255\n",
    "\n",
    "#만약 감정을 분노와 무표정에 더 많이 추가한다면 카테고리 2에서 n으로 수정하면 됩니다.\n",
    "Y_train = np_utils.to_categorical(Y_train, 2)\n",
    "Y_test = np_utils.to_categorical(Y_test, 2)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(3,3), input_shape = (128, 128, 1), activation = 'relu'))\n",
    "model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = 2))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(2, activation = 'softmax'))\n",
    "\n",
    "model.compile(loss = 'categorical_crossentropy', \n",
    "             optimizer = 'adam',\n",
    "             metrics = ['accuracy'])\n",
    "MODEL_DIR = './model/'\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    os.mkdir(MODEL_DIR)\n",
    "    \n",
    "modelpath = './model/{epoch:02d}-{val_loss:.4f}.hdf5'\n",
    "checkpointer = ModelCheckpoint(filepath = modelpath, monitor='val_loss', verbose = 1, save_best_only = True)\n",
    "early_stopping_callback = EarlyStopping(monitor = 'val_loss', patience=10)\n",
    "\n",
    "history = model.fit(X_train, Y_train, validation_data = (X_test, Y_test), epochs = 30, batch_size = 50, verbose = 0,\n",
    "                   callbacks = [early_stopping_callback, checkpointer])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
